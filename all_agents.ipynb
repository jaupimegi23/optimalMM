{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RandomAgent:\n",
    "    \n",
    "    env_d: int # number of ticks away from mid price the market maker can quote\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "\n",
    "    def get_action(self):\n",
    "        self.t += 1\n",
    "        selected_action_bid = np.random.randint(0, self.env_d)\n",
    "        selected_action_ask = np.random.randint(0, self.env_d)\n",
    "        return np.array([selected_action_bid, selected_action_ask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QAgent:\n",
    "    \n",
    "    # Environment params\n",
    "    env_d: int # number of ticks away from mid price the market maker can quote\n",
    "    env_Q: int # the maximum (absolute) allowed held volume\n",
    "    env_T: int # the number of time steps the model will be run for\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate: float = 0.1\n",
    "    gamma: float = 0.99\n",
    "    epsilon: float = 1.0\n",
    "    epsilon_min: float = 0.01\n",
    "    epsilon_decay: float = 0.3\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Initialize Q-table\n",
    "        self.q_table = {(i, j): np.zeros((self.env_d, self.env_d)) \n",
    "                        for i in range(-self.env_Q, self.env_Q + 1) \n",
    "                        for j in range(self.env_T + 1)}\n",
    "        # log\n",
    "        self.t = 0\n",
    "        self.log = []\n",
    "\n",
    "    def update_Q(self, action, reward, state, next_state):\n",
    "        # Update Q action-value given (action, reward)\n",
    "        self.q_table[state][action] += self.learning_rate * ((reward + self.gamma * (np.max(self.q_table[next_state]))) - self.q_table[state][action])\n",
    "        # record log\n",
    "        self.log.append({'t':self.t, \n",
    "                         'state':state, 'action':action, 'reward':reward, 'next_state':next_state,\n",
    "                         'q_table':self.q_table.copy()})\n",
    "   \n",
    "    def get_action(self, state):\n",
    "        self.t += 1\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Random explore\n",
    "            selected_action_bid = np.random.randint(0, self.env_d)\n",
    "            selected_action_ask = np.random.randint(0, self.env_d) \n",
    "            return np.array([selected_action_bid, selected_action_ask])\n",
    "        else:\n",
    "            # Best exploit\n",
    "            return np.array(np.unravel_index(np.argmax(self.q_table[state]), self.q_table[state].shape))\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "    def get_log(self, show=True):\n",
    "        log_df = pd.DataFrame(self.log)\n",
    "        if show: display(log_df)\n",
    "        return log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExploreFirstAgent:\n",
    "\n",
    "    num_actions_bid: int\n",
    "    num_actions_ask: int\n",
    "    max_explore: int\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # log\n",
    "        self.t = 0\n",
    "        self.log = []\n",
    "        # action counts n(a)\n",
    "        self.action_counts_bid = np.zeros(self.num_actions_bid, dtype=int) \n",
    "        self.action_counts_ask = np.zeros(self.num_actions_ask, dtype=int) \n",
    "        # action value Q(a)\n",
    "        self.Q_bid = np.zeros(self.num_actions_bid, dtype=float) \n",
    "        self.Q_ask = np.zeros(self.num_actions_ask, dtype=float) \n",
    "\n",
    "    def update_Q(self, action, reward):\n",
    "        # update Q action-value given (action, reward)\n",
    "        # bid\n",
    "        action_bid = action[0]\n",
    "        sum_reward = self.Q_bid[action_bid] * self.action_counts_bid[action_bid] + reward # Sum of reward\n",
    "        self.action_counts_bid[action_bid] = self.action_counts_bid[action_bid] + 1 # Update counts\n",
    "        self.Q_bid[action_bid] = sum_reward / self.action_counts_bid[action_bid] # Update Q\n",
    "        # ask\n",
    "        action_ask = action[1]\n",
    "        sum_reward = self.Q_ask[action_ask] * self.action_counts_ask[action_ask] + reward # Sum of reward\n",
    "        self.action_counts_ask[action_ask] = self.action_counts_ask[action_ask] + 1 # Update counts\n",
    "        self.Q_ask[action_ask] = sum_reward / self.action_counts_ask[action_ask] # Update Q\n",
    "        # record log\n",
    "        self.log.append({'t': self.t, \n",
    "                         'action_bid':action_bid, 'N_bid':self.action_counts_bid.copy(), 'Q_bid':self.Q_bid.copy(),\n",
    "                         'action_ask':action_ask, 'N_ask':self.action_counts_ask.copy(), 'Q_ask':self.Q_ask.copy()})\n",
    "\n",
    "    def get_action(self):\n",
    "        self.t += 1\n",
    "        if self.t <= self.max_explore: \n",
    "            # Random explore\n",
    "            selected_action_bid = np.random.randint(0, self.num_actions_bid)\n",
    "            selected_action_ask = np.random.randint(0, self.num_actions_ask) \n",
    "        else:\n",
    "            # Best exploit\n",
    "            selected_action_bid = np.argmax(self.Q_bid)\n",
    "            selected_action_ask = np.argmax(self.Q_ask)\n",
    "\n",
    "        return [selected_action_bid, selected_action_ask]   \n",
    "    \n",
    "    def get_log(self, show=True):\n",
    "        log_df = pd.DataFrame(self.log)\n",
    "        if show: display(log_df)\n",
    "        return log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class UCBAgent:\n",
    "    \n",
    "    num_actions_bid: int\n",
    "    num_actions_ask: int\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # log\n",
    "        self.t = 0\n",
    "        self.log = []\n",
    "        # action counts n(a)\n",
    "        self.action_counts_bid = np.zeros(self.num_actions_bid, dtype=int) \n",
    "        self.action_counts_ask = np.zeros(self.num_actions_ask, dtype=int) \n",
    "        # action value Q(a)\n",
    "        self.Q_bid = np.zeros(self.num_actions_bid, dtype=float) \n",
    "        self.Q_ask = np.zeros(self.num_actions_ask, dtype=float) \n",
    "\n",
    "    def update_Q(self, action, reward):\n",
    "        # Update Q action-value given (action, reward)\n",
    "        # bid\n",
    "        action_bid = action[0]\n",
    "        sum_reward = self.Q_bid[action_bid] * self.action_counts_bid[action_bid] + reward # Sum of reward\n",
    "        self.action_counts_bid[action_bid] = self.action_counts_bid[action_bid] + 1 # Update counts\n",
    "        self.Q_bid[action_bid] = sum_reward / self.action_counts_bid[action_bid] # Update Q\n",
    "        # ask\n",
    "        action_ask = action[1]\n",
    "        sum_reward = self.Q_ask[action_ask] * self.action_counts_ask[action_ask] + reward # Sum of reward\n",
    "        self.action_counts_ask[action_ask] = self.action_counts_ask[action_ask] + 1 # Update counts\n",
    "        self.Q_ask[action_ask] = sum_reward / self.action_counts_ask[action_ask] # Update Q\n",
    "        # record log\n",
    "        self.log.append({'t': self.t, \n",
    "                         'action_bid':action_bid, 'N_bid':self.action_counts_bid.copy(), 'Q_bid':self.Q_bid.copy(),\n",
    "                         'action_ask':action_ask, 'N_ask':self.action_counts_ask.copy(), 'Q_ask':self.Q_ask.copy()})\n",
    "   \n",
    "    def get_action(self):\n",
    "        self.t += 1\n",
    "        # Calculate the exploration bonus. To avoid a division by zero, add a small delta=1e-5 to the denominator\n",
    "        # bid\n",
    "        exploration_bonus_bid = np.zeros(self.num_actions_bid, dtype=float)\n",
    "        delta = 1e-5\n",
    "        for action_bid in range(self.num_actions_bid):\n",
    "            exploration_bonus_bid[action_bid] = np.sqrt(4 * np.log(self.t) / (self.action_counts_bid[action_bid] + delta))\n",
    "        Q_explore_bid = self.Q_bid + exploration_bonus_bid\n",
    "        selected_action_bid = np.argmax(Q_explore_bid)\n",
    "        # ask\n",
    "        exploration_bonus_ask = np.zeros(self.num_actions_ask, dtype=float)\n",
    "        delta = 1e-5\n",
    "        for action_ask in range(self.num_actions_ask):\n",
    "            exploration_bonus_ask[action_ask] = np.sqrt(4 * np.log(self.t) / (self.action_counts_ask[action_ask] + delta))\n",
    "        Q_explore_ask = self.Q_ask + exploration_bonus_ask\n",
    "        selected_action_ask = np.argmax(Q_explore_ask)\n",
    "\n",
    "        return [selected_action_bid, selected_action_ask]   \n",
    "    \n",
    "    def get_log(self, show=True):\n",
    "        log_df = pd.DataFrame(self.log)\n",
    "        if show: display(log_df)\n",
    "        return log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EpsilonGreedyAgent:\n",
    "    \n",
    "    num_actions_bid: int\n",
    "    num_actions_ask: int\n",
    "    epsilon: float = 0.1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # log\n",
    "        self.t = 0\n",
    "        self.log = []\n",
    "        # action counts n(a)\n",
    "        self.action_counts_bid = np.zeros(self.num_actions_bid, dtype=int) \n",
    "        self.action_counts_ask = np.zeros(self.num_actions_ask, dtype=int) \n",
    "        # action value Q(a)\n",
    "        self.Q_bid = np.zeros(self.num_actions_bid, dtype=float) \n",
    "        self.Q_ask = np.zeros(self.num_actions_ask, dtype=float) \n",
    "\n",
    "    def update_Q(self, action, reward):\n",
    "        # Update Q action-value given (action, reward)\n",
    "        # bid\n",
    "        action_bid = action[0]\n",
    "        sum_reward = self.Q_bid[action_bid] * self.action_counts_bid[action_bid] + reward # Sum of reward\n",
    "        self.action_counts_bid[action_bid] = self.action_counts_bid[action_bid] + 1 # Update counts\n",
    "        self.Q_bid[action_bid] = sum_reward / self.action_counts_bid[action_bid] # Update Q\n",
    "        # ask\n",
    "        action_ask = action[1]\n",
    "        sum_reward = self.Q_ask[action_ask] * self.action_counts_ask[action_ask] + reward # Sum of reward\n",
    "        self.action_counts_ask[action_ask] = self.action_counts_ask[action_ask] + 1 # Update counts\n",
    "        self.Q_ask[action_ask] = sum_reward / self.action_counts_ask[action_ask] # Update Q\n",
    "        # record log\n",
    "        self.log.append({'t': self.t, \n",
    "                         'action_bid':action_bid, 'N_bid':self.action_counts_bid.copy(), 'Q_bid':self.Q_bid.copy(),\n",
    "                         'action_ask':action_ask, 'N_ask':self.action_counts_ask.copy(), 'Q_ask':self.Q_ask.copy()})\n",
    "   \n",
    "    def get_action(self):\n",
    "        self.t += 1\n",
    "        # Epsilon-greedy policy\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Random explore\n",
    "            selected_action_bid = np.random.randint(0, self.num_actions_bid)\n",
    "            selected_action_ask = np.random.randint(0, self.num_actions_ask) \n",
    "        else:\n",
    "            # Best exploit\n",
    "            selected_action_bid = np.argmax(self.Q_bid)\n",
    "            selected_action_ask = np.argmax(self.Q_ask)\n",
    "\n",
    "        return [selected_action_bid, selected_action_ask]   \n",
    "    \n",
    "    def get_log(self, show=True):\n",
    "        log_df = pd.DataFrame(self.log)\n",
    "        if show: display(log_df)\n",
    "        return log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecayEpsilonGreedyAgent:\n",
    "    \n",
    "    num_actions_bid: int\n",
    "    num_actions_ask: int\n",
    "    epsilon: float = 0.5\n",
    "    epsilon_min: float = 0.01\n",
    "    epsilon_decay: float = 0.01\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # log\n",
    "        self.t = 0\n",
    "        self.log = []\n",
    "        # action counts n(a)\n",
    "        self.action_counts_bid = np.zeros(self.num_actions_bid, dtype=int) \n",
    "        self.action_counts_ask = np.zeros(self.num_actions_ask, dtype=int) \n",
    "        # action value Q(a)\n",
    "        self.Q_bid = np.zeros(self.num_actions_bid, dtype=float) \n",
    "        self.Q_ask = np.zeros(self.num_actions_ask, dtype=float) \n",
    "\n",
    "    def update_Q(self, action, reward):\n",
    "        # Update Q action-value given (action, reward)\n",
    "        # bid\n",
    "        action_bid = action[0]\n",
    "        sum_reward = self.Q_bid[action_bid] * self.action_counts_bid[action_bid] + reward # Sum of reward\n",
    "        self.action_counts_bid[action_bid] = self.action_counts_bid[action_bid] + 1 # Update counts\n",
    "        self.Q_bid[action_bid] = sum_reward / self.action_counts_bid[action_bid] # Update Q\n",
    "        # ask\n",
    "        action_ask = action[1]\n",
    "        sum_reward = self.Q_ask[action_ask] * self.action_counts_ask[action_ask] + reward # Sum of reward\n",
    "        self.action_counts_ask[action_ask] = self.action_counts_ask[action_ask] + 1 # Update counts\n",
    "        self.Q_ask[action_ask] = sum_reward / self.action_counts_ask[action_ask] # Update Q\n",
    "        # record log\n",
    "        self.log.append({'t': self.t, \n",
    "                         'action_bid':action_bid, 'N_bid':self.action_counts_bid.copy(), 'Q_bid':self.Q_bid.copy(),\n",
    "                         'action_ask':action_ask, 'N_ask':self.action_counts_ask.copy(), 'Q_ask':self.Q_ask.copy()})\n",
    "   \n",
    "    def get_action(self):\n",
    "        self.t += 1\n",
    "        # Epsilon-greedy policy\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Random explore\n",
    "            selected_action_bid = np.random.randint(0, self.num_actions_bid)\n",
    "            selected_action_ask = np.random.randint(0, self.num_actions_ask) \n",
    "        else:\n",
    "            # Best exploit\n",
    "            selected_action_bid = np.argmax(self.Q_bid)\n",
    "            selected_action_ask = np.argmax(self.Q_ask)\n",
    "\n",
    "        return [selected_action_bid, selected_action_ask]  \n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_decay) \n",
    "    \n",
    "    def get_log(self, show=True):\n",
    "        log_df = pd.DataFrame(self.log)\n",
    "        if show: display(log_df)\n",
    "        return log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalAgent:\n",
    "    def __init__(self, env):\n",
    "        self.T = env.T\n",
    "        self.Q = env.Q\n",
    "        self.dp = env.dp\n",
    "        self.phi = env.phi\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.log = []\n",
    "\n",
    "    def generate_optimal_depth(self, env, bid=True):\n",
    "        data = []\n",
    "\n",
    "        q_s = np.arange(start=-env.Q, stop=env.Q + 1)\n",
    "\n",
    "        for q in q_s:\n",
    "            data_q = []\n",
    "            for t in range(self.T + 1):\n",
    "                env.t = t\n",
    "                env.Q_t = q\n",
    "                depth = env.calc_analytically_optimal()[1 - bid]\n",
    "                #depth = env.transform_action(env.discrete_analytically_optimal())[1 - bid] * (1 - 2 * bid)\n",
    "\n",
    "                data_q.append(depth)\n",
    "\n",
    "            data.append(data_q)\n",
    "\n",
    "        return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def act(self, state, memory):\n",
    "        state = torch.from_numpy(np.array(state)).float().to(device)\n",
    "        #state = torch.from_numpy(state).float().to(device) \n",
    "        probs = F.softmax(self.actor(state), dim=-1)\n",
    "        value = self.critic(state)\n",
    "        \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(dist.log_prob(action))\n",
    "        memory.values.append(value)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        state = state.float()\n",
    "        action = action.float()\n",
    "        \n",
    "        probs = F.softmax(self.actor(state), dim=-1)\n",
    "        value = self.critic(state)\n",
    "        \n",
    "        dist = Categorical(probs)\n",
    "        \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        \n",
    "        return action_logprobs, torch.squeeze(value), dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.log = []\n",
    "    \n",
    "    def update(self, memory):\n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, done in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if done:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        rewards = rewards.float()\n",
    "        \n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(memory.states).to(device).detach()\n",
    "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
    "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
    "        \n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values :\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "            advantages = rewards - state_values.detach()   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = loss.float()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "        self.values = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "        del self.values[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
